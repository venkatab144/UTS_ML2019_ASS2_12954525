{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_A2_12954525.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatab144/UTS_ML2019_ASS2_12954525/blob/master/ML_A2_12954525.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3-YErxNIDGZ",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq94NboSIJr-",
        "colab_type": "text"
      },
      "source": [
        "The CART (Classification And Regression Trees) algorithm, classifies data by splitting attributes based on their gini index. CART starts with the root node, and generates a split by asking a question which results in the highest information gain. This is done by calculating the Gini index of the dataset from the root node, and then subtracting the weighted average of the Gini index of the resulting datasets from the split.\n",
        "\n",
        "The Gini index of a dataset is calculated by calculating the probability that a randomly chosen label within a given amount of rows is correct. This can be used to measure the impurity of data. A Gini index of 1 means that there is a 0% chance of incorrectly assigning a label and that the data is pure. A low Gini index means that there is a higher risk of assigning an incorrect label to a row and this results in a higher Gini impurity. Therefore, it can be said that impurity is inversely proportional to the Gini index.\n",
        "\n",
        "The inputs for this CART implementation is an array of rows which contain attributes and a label in the final column. The ouput will be the classes that were predicted from the tree and confidence (in %) of the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-65v_S-IKLV",
        "colab_type": "text"
      },
      "source": [
        "# Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seVmKya0IKWt",
        "colab_type": "text"
      },
      "source": [
        "**Relevant Terminology:**\n",
        "\n",
        "**Gini Index:**\n",
        "- Gini index measures the impurity of a dataset by calculating the probablilty that a row of data belongs to a certain label/class.\n",
        "\n",
        "**Root Node:**\n",
        "- The starting node which contains the entire dataset and is split into child nodes.\n",
        "\n",
        "**Parent Node:**\n",
        "- A node which contains child nodes.\n",
        "\n",
        "**Child Node:**\n",
        "- A node which is created when a parent node is split.\n",
        "\n",
        "**Splitting:**\n",
        "- The process of dividing the dataset into subsets based off a question, and results in new sub-trees.\n",
        "\n",
        "**Question:**\n",
        "- Gives a true/false answer based on a certain condition which is based on an attribute.\n",
        "\n",
        "**Sub-tree:**\n",
        "- A new tree created from splitting a node.\n",
        "\n",
        "**Leaf Node:**\n",
        "- A terminal node at the end of a tree/sub-tree. It contains the predicted labels for a class.\n",
        "\n",
        "The structure for the CART algorithm is as follows:\n",
        "1. The Gini index of the entire dataset for the root node is calculated using the following formula:\n",
        "<center>GINI FORMULA</center>\n",
        "2. A \"question\" is proposed based off each attribute and the question with the highest information gain is selected.\n",
        "3. True and False nodes are created for this split.\n",
        "4. Step 1 is recursivley called on these nodes until a <i>leaf</i> node is reached.\n",
        "> A leaf node is the terminal node for a specific branch of a tree. It contains the predicted labels for the row when that branch is followed. A leaf node only exists when the information gain is 0 and there are no further questions to ask.\n",
        "5. Once all leaf nodes have been created and no more useful (questions that have some information gain), have been asked, the model has completed training and is ready to classify data.\n",
        "\n",
        "Planned Approach/Structure:\n",
        "1. Starting from the root node, go through every attribute and generate a question.\n",
        "2. To generate a question, we first check if the value is numeric or not.\n",
        "3. If the value is numeric, we generate a question to check if all other numeric attributes are greater than or equal to its value\n",
        "4. If the value is non-numeric, we check if all other attributes are an exact match.\n",
        "5. We calculate the Gini impurity of the current node.\n",
        "6. We then split the data based on the question and generate true and false rows.\n",
        "7. We calculate the weighted average of the Gini impurity of these true/false nodes.\n",
        "8. We the calculate the information gain by subtracting the Gini impurity of the true/false child nodes from the parent node, and store the question with the highest information gain.\n",
        "9. Once we have the best possible split, if the information gain is 0, that means no new information can be gained and a leaf node is created.\n",
        "10. If there is information to be gained, we recursivley create new trees from the true and false branches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NDaID-HIKkf",
        "colab_type": "text"
      },
      "source": [
        "# Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYwFnR8aIKuF",
        "colab_type": "text"
      },
      "source": [
        "## !!!!! This section will only list and explain each method. A full, working and testable implementation will be located at the bottom of this notebook in the \"*Full Implementation*\" section. !!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAIucw4u2ynH",
        "colab_type": "text"
      },
      "source": [
        "## Main Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofe05OKK254W",
        "colab_type": "text"
      },
      "source": [
        "To start the process, the algorithm needs to take the entire training dataset and start constructing the full tree, starting with the root node. It then needs to find the best query that splits the node resulting in the most information gain and then creates sub-trees from the split node. This can be seen in the `create_tree()` method below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAw81deD6MN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tree(self, data_rows, is_root_node=False):\n",
        "        # We need to find the split that results in the lowest Gini index and\n",
        "        # the highest information gain by proposing a question from the given data_rows\n",
        "        \n",
        "        # Create a new node object to store the current node\n",
        "        current_node = self.Node(data_rows=data_rows, is_root_node=is_root_node)\n",
        "        \n",
        "        # Calculate the split which produces the best information gain and store\n",
        "        # the query, so that we know what needs to be asked where\n",
        "        best_gain, best_query = self.find_best_split(current_node)\n",
        "        \n",
        "        # If the split results in 0 gain, this means that there is no more information\n",
        "        # to be gained from this node and we can store it as a Terminal node.\n",
        "        if (best_gain == 0):\n",
        "            current_node.populate_node(best_gain, best_query, None, None)\n",
        "            \n",
        "            # Get the labels that are left in the split dataset and store them \n",
        "            # as the predictions\n",
        "            predicted_labels, predicted_counts = self.get_label_counts(current_node)\n",
        "            current_node.predictions = predicted_labels\n",
        "        else:\n",
        "            # If there IS information to be gained, we need to store the true\n",
        "            # and false nodes and create new sub-trees from them\n",
        "            true_rows, false_rows = self.create_split(data_rows, best_query)\n",
        "            \n",
        "            true_node = self.create_tree(true_rows.data_rows)\n",
        "            false_node = self.create_tree(false_rows.data_rows)\n",
        "            \n",
        "            # Populate the node with details for visualization\n",
        "            current_node.populate_node(best_gain, best_query, true_node, false_node)\n",
        "        \n",
        "        # Once we've finished our recursion and end up back at the root node\n",
        "        # we can store the root node in a variable to access it later for prediction\n",
        "        if (current_node.is_root_node):\n",
        "            print(\"Finished\")\n",
        "            self.trained_model = current_node\n",
        "        \n",
        "        return current_node"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uj9LuZu77pv",
        "colab_type": "text"
      },
      "source": [
        "Next we need to be able to find what the best query is for the highest information gain. This is achieved by iterating through all attributes and their values, creating a query and splitting the data based on if the other values match it. We then calculate the Gini impurity using the following formula: $$Gini(X) = 1-\\sum_{i=1}^C (p_{i})^2$$ where $p_{i}$ is the prbability that row $X$ belongs to class $C$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oHgSsNe_G_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_gini_impurity(self, node):\n",
        "    # Get the labels and their counts for calculation and index purposes\n",
        "    labels, counts = self.get_label_counts(node)\n",
        "    \n",
        "    # We need to know the total number of rows to calculate the probablity that\n",
        "    # a class belongs to a row\n",
        "    num_rows = node.num_rows()\n",
        "    impurity = 1\n",
        "    \n",
        "    # Calculate the Gini impurity of that node.\n",
        "    for label in labels:\n",
        "        label_probabililty = counts[label] / float(num_rows)\n",
        "        impurity -= label_probabililty**2\n",
        "\n",
        "    return impurity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyLiY-yo_G6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_best_split(self, parent_node):\n",
        "        # We need to store the best gain and the best query so we can correctly path\n",
        "        # values from predictions\n",
        "        best_info_gain = 0\n",
        "        best_query = None\n",
        "        \n",
        "        # We need the parent node gini impurity to be able to calculate the information\n",
        "        # gain\n",
        "        parent_gini = self.calculate_gini_impurity(parent_node)\n",
        "        \n",
        "        # We need to now start constructing queries for every single attribute and\n",
        "        # their values in order to find the best query to split the node with\n",
        "        for column in parent_node.attribute_columns():\n",
        "            # Gets all the unique values for that column\n",
        "            for value in parent_node.data_rows[column].unique():\n",
        "            \n",
        "                query = self.Query(column, value)\n",
        "                \n",
        "                # Create a split and generate the true and false nodes so that we\n",
        "                # can evaluate the information gain\n",
        "                true_node, false_node = self.create_split(parent_node.data_rows, query)\n",
        "                \n",
        "                # If the split generates an empty true/false column we skip it as \n",
        "                # its redundant and does not help us\n",
        "                if (true_node.num_rows() > 0 and false_node.num_rows() > 0):\n",
        "                    # We need to know the info gain of this split so we can store\n",
        "                    # the best split\n",
        "                    info_gain = self.calculate_info_gain(parent_gini, true_node, false_node);\\\n",
        "    \n",
        "                    # Store the best gain\n",
        "                    if (info_gain >= best_info_gain):\n",
        "                        best_info_gain = info_gain\n",
        "                        best_query = query\n",
        "                    \n",
        "        return best_info_gain, best_query    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7f5Qyk0_zf5",
        "colab_type": "text"
      },
      "source": [
        "We need to also calculate the information gain by subtracting the weighted average of true and false nodes from the parent Gini impurity, and that is done through: $$g_{total} = g_{parent} - (p_{true} * g_{true}) - (p_{false} * g_{false})$$ where $g$ is gain and $p$ is how often it appears in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7yaWs5bBWEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_info_gain(self, parent_gini, true_node, false_node):\n",
        "    # Calculate the Gini impurity of the true node and false node\n",
        "    gini_true = self.calculate_gini_impurity(true_node)\n",
        "    gini_false = self.calculate_gini_impurity(false_node)\n",
        "    \n",
        "    # Calculate the weights of each node\n",
        "    true_weight = float(true_node.num_rows()) / (true_node.num_rows() + false_node.num_rows())\n",
        "    false_weight = 1 - true_weight\n",
        "    \n",
        "    # Calculate the total information gain\n",
        "    info_gain = parent_gini - ((true_weight * gini_true) + (false_weight * gini_false))\n",
        "    \n",
        "    return info_gain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hozit_oB67m",
        "colab_type": "text"
      },
      "source": [
        "A split needs to be create which evaluates the query for each row and creates the corresponding true/false nodes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ6GxOVwCBOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_split(self, data_rows, query):\n",
        "    # New nodes for true/false branches\n",
        "    true_node, false_node = self.Node(pd.DataFrame()), self.Node(pd.DataFrame())\n",
        "\n",
        "    # If a row matches the query, append it to the true node and if it doesnt,\n",
        "    # append it to the false node\n",
        "    for index, row in data_rows.iterrows():\n",
        "        if (query.evaluate_result(row)):\n",
        "            true_node.append_row(row)\n",
        "        else:\n",
        "            false_node.append_row(row)\n",
        "        \n",
        "    return true_node, false_node"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlkqCqozCzFu",
        "colab_type": "text"
      },
      "source": [
        "## Fit, Classify and Predict methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UuanWETDItm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(self, training_data):\n",
        "    # Initialize the the root node and recursion process\n",
        "    return self.create_tree(training_data, True)\n",
        "    \n",
        "def classify(self, row, tree):\n",
        "    # If we have reached a prediction then display the results\n",
        "    if (tree.is_terminal_node()):\n",
        "        print(\"Actual:\", row[-1], \"Predicted:\", tree.predictions[0])\n",
        "        return tree.predictions\n",
        "    \n",
        "    # Otherwise match the row with the query and follow the true/false branch\n",
        "    if (tree.query.evaluate_result(row)):\n",
        "        return self.classify(row, tree.true_node)\n",
        "    else:\n",
        "        return self.classify(row, tree.false_node)\n",
        "\n",
        "def predict(self, data_rows):\n",
        "    # Prevent prediction from an incomplete model\n",
        "    if (self.trained_model is None):\n",
        "        print(\"Please train the model with fit(training_data) first.\")\n",
        "        return\n",
        "    \n",
        "    # Otherwise, classify each row.\n",
        "    for row in data_rows.iterrows():\n",
        "        self.classify(row[1], self.trained_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFSyNEeiDPEe",
        "colab_type": "text"
      },
      "source": [
        "## Query and Node Sub-classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnXEqEJJDUqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, data_rows, is_root_node=False):\n",
        "        self.data_rows = data_rows\n",
        "        self.query = None\n",
        "        self.info_gain = 0\n",
        "        self.true_node = None\n",
        "        self.false_node = None\n",
        "        self.node_type = \"\"\n",
        "        self.is_root_node = is_root_node\n",
        "        self.predictions = None\n",
        "        \n",
        "    def populate_node(self, gain, query, true_node, false_node):\n",
        "        self.info_gain = gain\n",
        "        self.query = query\n",
        "        self.true_node = true_node\n",
        "        self.false_node = false_node\n",
        "        \n",
        "        if (gain == 0):\n",
        "            self.node_type = \"Terminal\"\n",
        "        else:\n",
        "            self.node_type = \"Split\"\n",
        "        \n",
        "    def num_rows(self):\n",
        "        return self.data_rows.shape[0]\n",
        "    \n",
        "    def attribute_columns(self):\n",
        "        return self.data_rows.columns[:-1]\n",
        "    \n",
        "    def label_column(self):\n",
        "        return self.data_rows.columns[-1]\n",
        "    \n",
        "    def append_row(self, row):\n",
        "        self.data_rows = self.data_rows.append(row)\n",
        "        \n",
        "    def is_terminal_node(self):\n",
        "        return self.node_type == \"Terminal\"\n",
        "    \n",
        "    def is_split_node(self):\n",
        "        return self.node_type == \"Split\"\n",
        "\n",
        "    class Query:\n",
        "        def __init__(self, attribute, value):\n",
        "            self.attribute = attribute\n",
        "            self.value = value\n",
        "        \n",
        "        def is_number(self, object_to_check):\n",
        "            return isinstance(object_to_check, Number)\n",
        "            \n",
        "        def evaluate_result(self, row):\n",
        "            value_to_evaluate = row[self.attribute]\n",
        "            \n",
        "            if(self.is_number(value_to_evaluate)):\n",
        "                return value_to_evaluate >= self.value\n",
        "            else:\n",
        "                return value_to_evaluate == self.value\n",
        "        \n",
        "        def query_desc(self):\n",
        "            desc = self.attribute\n",
        "            if(self.is_number(self.value)):\n",
        "                desc += \" >= \"\n",
        "            else:\n",
        "                desc += \" == \"\n",
        "            desc += str(self.value)\n",
        "            \n",
        "            return desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqHQrdXADeYZ",
        "colab_type": "text"
      },
      "source": [
        "## Helper Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpqYYIh_Dgl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Logger:\n",
        "    def __init__(self):\n",
        "        self.logs = []\n",
        "    \n",
        "    def log(self, *args):\n",
        "        self.logs.append(' '.join(map(str, args)))\n",
        "        \n",
        "    def print_logs(self):\n",
        "        for log in self.logs:\n",
        "            print(log)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWHzCllSEgSo",
        "colab_type": "text"
      },
      "source": [
        "## Usage\n",
        "### !!! Please check \"*Full Implementation*\" section for a working solution. !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgr5OI-HEiLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = DecisionTreeClassifier()\n",
        "\n",
        "classifier.fit(training_data)\n",
        "classifier.predict(testing_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtOgkEC-Fu4A",
        "colab_type": "text"
      },
      "source": [
        "## Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOhdG6YEFwxg",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "Finished\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: virginica Predicted: virginica\n",
        "Actual: setosa Predicted: setosa\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: virginica Predicted: virginica\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: virginica Predicted: virginica\n",
        "Actual: setosa Predicted: setosa\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: setosa Predicted: setosa\n",
        "Actual: virginica Predicted: virginica\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: virginica Predicted: virginica\n",
        "Actual: setosa Predicted: setosa\n",
        "Actual: virginica Predicted: virginica\n",
        "Actual: virginica Predicted: virginica\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: setosa Predicted: setosa\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: setosa Predicted: setosa\n",
        "Actual: virginica Predicted: virginica\n",
        "Actual: setosa Predicted: setosa\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: virginica Predicted: virginica\n",
        "Actual: setosa Predicted: setosa\n",
        "Actual: versicolor Predicted: versicolor\n",
        "Actual: setosa Predicted: setosa \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2dqmJW0IK37",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSAHHHUUILBr",
        "colab_type": "text"
      },
      "source": [
        "When tested on the Iris dataset using a training/test split of 80:20, the implementation presented in this report achieves a 100% accuracy rate. However, the model that is generated by the above implementation will be heavily over-fitted to whatever dataset it is trained with, and any new data will require the model to be re-trained in order to maintain its accuracy.\n",
        "\n",
        "A potential solution to this is utilizing k-fold cross validation in order to generate multiple folds of the training set, allowing the model to be trained and tested on a 'k' number of different training/testing sets. This will help reduce the presence over-fitting and will reduce the impact that new data will have on an already trained model.\n",
        "\n",
        "The algorithm presented is also not optimised to achieve the best performance and is not the most efficient. It is a basic implementation of the CART algorithm and as such, does not include methods such as pruning and max-depth configuration. This specific implementation of the CART algorithm utilizes the pandas library and makes use of DataFrames heavliy which are more computationally expensive than using simple lists. It is also quite object-oriented; nodes and queries are stored in their own objects and occupy unnecessary memory. Objects that end up having no use are created in abundance (e.g. in find_best_split() true/false nodes are created for every possible attribute/value combination, as well as questions), without being properly destroyed, resulting in heavy memory usage. Proper garbage collection was not able to be achieved in the time frame imposed on this assignment. \n",
        "\n",
        "Decision Tree (DT) algorithms such as CART are white box learning algorithms used for classification and regression. The internal logic used by the DT can be viewed unlike other black box algorithms. Compared to other classification algorithms such as: KNN and SVM, the process followed by the is viewable and it is often possible to construct flowcharts of all the possible node paths leading to which prediction was made. There are many flaws of decision tree algorithms such as CART, small changes can have massive impacts on the model, larger datasets make it extremely computationally expensive and at times it can make flawed decision and assumptions causing incorrect predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6Uggn3RILPF",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGAUGKzVILaa",
        "colab_type": "text"
      },
      "source": [
        "In conclusion, through the implementation above, it can be seen that the CART algorithm can be successfully used to classify data accurately. However, limitations have also been outlined and there are many possible areas for improvement, such as: the use of k-fold cross validation to reduce over-fitting,using simple lists instead of DataFrames, pruning and max-dept customization, proper garbage collection and a reduced reliance on object-orientation.\n",
        "\n",
        "Implementing these improvements could very likely increase the efficiency of the algorithm and also increase its accuracy. Given more time, this algorithm could be optimised to perform faster, use less memory and increase its accuracy. This implementation would still not be suitable to larger datasets. If a problem exists that can be solved with decision tree classifiers it would be more worthy to use existing libraries such as scikit-learn, but, if full control and customization is required, like in an enterprise environment, it would be worthwhile to implement the algorithm from scratch and build to to suit the required needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFbkin1_ILlg",
        "colab_type": "text"
      },
      "source": [
        "# Ethical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-m3q4jfILvx",
        "colab_type": "text"
      },
      "source": [
        "The CART algorithm is a Supervised Learning machine learning task. There are many potential areas for misuse in the Machine Learning field. Recently, the concept of Deepfakes have surfaced and it has been shown that Deepfakes can successfully be used to superimpose an individuals face onto anothers body. Deepfakes employ the Generative Adversarial Networks (GAN) machine learning technique in order to be used in a malicous manner. The GAN technique utilizes supervized learning methods such as classification and regression, which is what the CART algorithm specializes in (Classification and Regression Trees).\n",
        "\n",
        "The Utilitarian approach to ethics weighs the value of an action based on the outcome is achieves. The CART algorithm used for unlawful actions would produce an unethical and unlawful outcome that would hurt the individuals that are affected by it. \n",
        "\n",
        "The Deontological Kantian Based approach argues that rather than analysing the affects and repercussions of an action to weigh whether it was right or wrong, that one should instead query the motives of the individual or entity that performed the action. Applied in this context, if the CART algorithm is used in a malicious way, the ethical problem lies not with the original author but with the person/enitity that used the algorithm for their unethical motives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwSk3fU8IL8n",
        "colab_type": "text"
      },
      "source": [
        "# Video Pitch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp99JAoFIMGR",
        "colab_type": "text"
      },
      "source": [
        "Highlights Challenges and Effort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJDPnKKUEzLC",
        "colab_type": "text"
      },
      "source": [
        "# Full Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77_mZDf4FMsr",
        "colab_type": "text"
      },
      "source": [
        "### Please Note: This code is not optimised and can take upto a minute to complete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N54qmnpmE1_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "54b597b1-5edc-48f9-afb0-cc0deb590f7e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numbers import Number\n",
        "\n",
        "iris = sns.load_dataset('iris')\n",
        "\n",
        "training_data, testing_data = train_test_split(iris, test_size=0.2)\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self):\n",
        "        self.logs = []\n",
        "    \n",
        "    def log(self, *args):\n",
        "        self.logs.append(' '.join(map(str, args)))\n",
        "        \n",
        "    def print_logs(self):\n",
        "        for log in self.logs:\n",
        "            print(log)\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self):\n",
        "        self.trained_model= None\n",
        "\n",
        "    def get_label_counts(self, node):\n",
        "        counts = node.data_rows.iloc[:,-1].value_counts();\n",
        "        labels = counts.index;\n",
        "        return labels, counts\n",
        "    \n",
        "    class Node:\n",
        "        def __init__(self, data_rows, is_root_node=False):\n",
        "            self.data_rows = data_rows\n",
        "            self.query = None\n",
        "            self.info_gain = 0\n",
        "            self.true_node = None\n",
        "            self.false_node = None\n",
        "            self.node_type = \"\"\n",
        "            self.is_root_node = is_root_node\n",
        "            self.predictions = None\n",
        "            \n",
        "        def populate_node(self, gain, query, true_node, false_node):\n",
        "            self.info_gain = gain\n",
        "            self.query = query\n",
        "            self.true_node = true_node\n",
        "            self.false_node = false_node\n",
        "            \n",
        "            if (gain == 0):\n",
        "                self.node_type = \"Terminal\"\n",
        "            else:\n",
        "                self.node_type = \"Split\"\n",
        "            \n",
        "        def num_rows(self):\n",
        "            return self.data_rows.shape[0]\n",
        "        \n",
        "        def attribute_columns(self):\n",
        "            return self.data_rows.columns[:-1]\n",
        "        \n",
        "        def label_column(self):\n",
        "            return self.data_rows.columns[-1]\n",
        "        \n",
        "        def append_row(self, row):\n",
        "            self.data_rows = self.data_rows.append(row)\n",
        "            \n",
        "        def is_terminal_node(self):\n",
        "            return self.node_type == \"Terminal\"\n",
        "        \n",
        "        def is_split_node(self):\n",
        "            return self.node_type == \"Split\"\n",
        "            \n",
        "    \n",
        "    def calculate_gini_impurity(self, node):\n",
        "        # Get the labels and their counts for calculation and index purposes\n",
        "        labels, counts = self.get_label_counts(node)\n",
        "        \n",
        "        # We need to know the total number of rows to calculate the probablity that\n",
        "        # a class belongs to a row\n",
        "        num_rows = node.num_rows()\n",
        "        impurity = 1\n",
        "        \n",
        "        # Calculate the Gini impurity of that node.\n",
        "        for label in labels:\n",
        "            label_probabililty = counts[label] / float(num_rows)\n",
        "            impurity -= label_probabililty**2\n",
        "    \n",
        "        return impurity\n",
        "    \n",
        "    class Query:\n",
        "        def __init__(self, attribute, value):\n",
        "            self.attribute = attribute\n",
        "            self.value = value\n",
        "        \n",
        "        def is_number(self, object_to_check):\n",
        "            return isinstance(object_to_check, Number)\n",
        "            \n",
        "        def evaluate_result(self, row):\n",
        "            value_to_evaluate = row[self.attribute]\n",
        "            \n",
        "            if(self.is_number(value_to_evaluate)):\n",
        "                return value_to_evaluate >= self.value\n",
        "            else:\n",
        "                return value_to_evaluate == self.value\n",
        "        \n",
        "        def query_desc(self):\n",
        "            desc = self.attribute\n",
        "            if(self.is_number(self.value)):\n",
        "                desc += \" >= \"\n",
        "            else:\n",
        "                desc += \" == \"\n",
        "            desc += str(self.value)\n",
        "            \n",
        "            return desc\n",
        "            \n",
        "            \n",
        "    def create_split(self, data_rows, query):\n",
        "        # New nodes for true/false branches\n",
        "        true_node, false_node = self.Node(pd.DataFrame()), self.Node(pd.DataFrame())\n",
        "\n",
        "        # If a row matches the query, append it to the true node and if it doesnt,\n",
        "        # append it to the false node\n",
        "        for index, row in data_rows.iterrows():\n",
        "            if (query.evaluate_result(row)):\n",
        "                true_node.append_row(row)\n",
        "            else:\n",
        "                false_node.append_row(row)\n",
        "         \n",
        "        return true_node, false_node\n",
        "    \n",
        "    def calculate_info_gain(self, parent_gini, true_node, false_node):\n",
        "        # Calculate the Gini impurity of the true node and false node\n",
        "        gini_true = self.calculate_gini_impurity(true_node)\n",
        "        gini_false = self.calculate_gini_impurity(false_node)\n",
        "        \n",
        "        # Calculate the weights of each node\n",
        "        true_weight = float(true_node.num_rows()) / (true_node.num_rows() + false_node.num_rows())\n",
        "        false_weight = 1 - true_weight\n",
        "        \n",
        "        # Calculate the total information gain\n",
        "        info_gain = parent_gini - ((true_weight * gini_true) + (false_weight * gini_false))\n",
        "        \n",
        "        return info_gain\n",
        "    \n",
        "    def find_best_split(self, parent_node):\n",
        "        # We need to store the best gain and the best query so we can correctly path\n",
        "        # values from predictions\n",
        "        best_info_gain = 0\n",
        "        best_query = None\n",
        "        \n",
        "        # We need the parent node gini impurity to be able to calculate the information\n",
        "        # gain\n",
        "        parent_gini = self.calculate_gini_impurity(parent_node)\n",
        "        \n",
        "        # We need to now start constructing queries for every single attribute and\n",
        "        # their values in order to find the best query to split the node with\n",
        "        for column in parent_node.attribute_columns():\n",
        "            # Gets all the unique values for that column\n",
        "            for value in parent_node.data_rows[column].unique():\n",
        "            \n",
        "                query = self.Query(column, value)\n",
        "                \n",
        "                # Create a split and generate the true and false nodes so that we\n",
        "                # can evaluate the information gain\n",
        "                true_node, false_node = self.create_split(parent_node.data_rows, query)\n",
        "                \n",
        "                # If the split generates an empty true/false column we skip it as \n",
        "                # its redundant and does not help us\n",
        "                if (true_node.num_rows() > 0 and false_node.num_rows() > 0):\n",
        "                    # We need to know the info gain of this split so we can store\n",
        "                    # the best split\n",
        "                    info_gain = self.calculate_info_gain(parent_gini, true_node, false_node);\\\n",
        "    \n",
        "                    # Store the best gain\n",
        "                    if (info_gain >= best_info_gain):\n",
        "                        best_info_gain = info_gain\n",
        "                        best_query = query\n",
        "                    \n",
        "        return best_info_gain, best_query    \n",
        "        \n",
        "    \n",
        "    def create_tree(self, data_rows, is_root_node=False):\n",
        "        # We need to find the split that results in the lowest Gini index and\n",
        "        # the highest information gain by proposing a question from the given data_rows\n",
        "        \n",
        "        # Create a new node object to store the current node\n",
        "        current_node = self.Node(data_rows=data_rows, is_root_node=is_root_node)\n",
        "        \n",
        "        # Calculate the split which produces the best information gain and store\n",
        "        # the query, so that we know what needs to be asked where\n",
        "        best_gain, best_query = self.find_best_split(current_node)\n",
        "        \n",
        "        # If the split results in 0 gain, this means that there is no more information\n",
        "        # to be gained from this node and we can store it as a Terminal node.\n",
        "        if (best_gain == 0):\n",
        "            current_node.populate_node(best_gain, best_query, None, None)\n",
        "            \n",
        "            # Get the labels that are left in the split dataset and store them \n",
        "            # as the predictions\n",
        "            predicted_labels, predicted_counts = self.get_label_counts(current_node)\n",
        "            current_node.predictions = predicted_labels\n",
        "        else:\n",
        "            # If there IS information to be gained, we need to store the true\n",
        "            # and false nodes and create new sub-trees from them\n",
        "            true_rows, false_rows = self.create_split(data_rows, best_query)\n",
        "            \n",
        "            true_node = self.create_tree(true_rows.data_rows)\n",
        "            false_node = self.create_tree(false_rows.data_rows)\n",
        "            \n",
        "            # Populate the node with details for visualization\n",
        "            current_node.populate_node(best_gain, best_query, true_node, false_node)\n",
        "        \n",
        "        # Once we've finished our recursion and end up back at the root node\n",
        "        # we can store the root node in a variable to access it later for prediction\n",
        "        if (current_node.is_root_node):\n",
        "            print(\"Finished\")\n",
        "            self.trained_model = current_node\n",
        "        \n",
        "        return current_node\n",
        "    \n",
        "    def fit(self, training_data):\n",
        "        # Initialize the the root node and recursion process\n",
        "        return self.create_tree(training_data, True)\n",
        "        \n",
        "    def classify(self, row, tree):\n",
        "        # If we have reached a prediction then display the results\n",
        "        if (tree.is_terminal_node()):\n",
        "            print(\"Actual:\", row[-1], \"Predicted:\", tree.predictions[0])\n",
        "            return tree.predictions\n",
        "        \n",
        "        # Otherwise match the row with the query and follow the true/false branch\n",
        "        if (tree.query.evaluate_result(row)):\n",
        "            return self.classify(row, tree.true_node)\n",
        "        else:\n",
        "            return self.classify(row, tree.false_node)\n",
        "    \n",
        "    def predict(self, data_rows):\n",
        "        # Prevent prediction from an incomplete model\n",
        "        if (self.trained_model is None):\n",
        "            print(\"Please train the model with fit(training_data) first.\")\n",
        "            return\n",
        "        \n",
        "        # Otherwise, classify each row.\n",
        "        for row in data_rows.iterrows():\n",
        "            self.classify(row[1], self.trained_model)\n",
        "\n",
        "classifier = DecisionTreeClassifier()\n",
        "\n",
        "classifier.fit(training_data)\n",
        "classifier.predict(testing_data)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: virginica Predicted: virginica\n",
            "Actual: setosa Predicted: setosa\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: virginica Predicted: virginica\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: virginica Predicted: virginica\n",
            "Actual: setosa Predicted: setosa\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: setosa Predicted: setosa\n",
            "Actual: virginica Predicted: virginica\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: virginica Predicted: virginica\n",
            "Actual: setosa Predicted: setosa\n",
            "Actual: virginica Predicted: virginica\n",
            "Actual: virginica Predicted: virginica\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: setosa Predicted: setosa\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: setosa Predicted: setosa\n",
            "Actual: virginica Predicted: virginica\n",
            "Actual: setosa Predicted: setosa\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: virginica Predicted: virginica\n",
            "Actual: setosa Predicted: setosa\n",
            "Actual: versicolor Predicted: versicolor\n",
            "Actual: setosa Predicted: setosa\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}